{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import MaxPooling1D,Flatten, Dense, LSTM, Conv1D, TimeDistributed, GRU\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import pandas as pd\n",
    "import model_fun\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "#https://machinelearningmastery.com/cnn-long-short-term-memory-networks/\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "import tensorflow as tf\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import itertools\n",
    "\n",
    "import model_fun as fun\n",
    "\n",
    "from adapt.feature_based import CORAL\n",
    "from adapt.instance_based import KLIEP\n",
    "from adapt.instance_based import KMM\n",
    "from adapt.instance_based import TrAdaBoostR2\n",
    "from adapt.instance_based import NearestNeighborsWeighting\n",
    "from adapt.feature_based import FA\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "import time\n",
    "import json\n",
    "\n",
    "import calendar\n",
    "\n",
    "!python --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Required Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_kpi(test_X,test_y,pred_result, title=\"NOTDEFINED\",additional_info=None,output=False):\n",
    "    rmse = float(format(np.sqrt(mean_squared_error(test_y, pred_result)), '.3f'))\n",
    "    mape = float(format(mean_absolute_percentage_error(test_y, pred_result), '.3f'))\n",
    "    max_min = test_y.max()-test_y.min()\n",
    "    nrmse = float(format(rmse / (max_min[0])*100, '.3f'))\n",
    "    if output:\n",
    "        print(\"-----------------------------------------\")\n",
    "        print(title)\n",
    "        print(\"RMSE:\\t\\t\\t\\t\"+ str(rmse))\n",
    "        print(\"NRMSE:\\t\\t\\t\\t\"+ str(nrmse))\n",
    "        print(\"MAPE:\\t\\t\\t\\t\"+ str(mape))\n",
    "        print(\"-----------------------------------------\")\n",
    "    \n",
    "    DICT_METRICS[title] = (rmse,nrmse,mape)\n",
    "    \n",
    "    if additional_info!=None:\n",
    "        DICT_METRICS[title] = (rmse,nrmse,mape)+additional_info\n",
    "    \n",
    "# functions needed to calculate similiar features (only select same features)\n",
    "def find_intersection(lists)-> list:\n",
    "    intersection = set(lists[0])\n",
    "    for li in lists[1:]:\n",
    "        intersection = intersection.intersection(li)\n",
    "    return list(intersection)\n",
    "\n",
    "def get_colname_intersection(dict_stations, li_station_names):\n",
    "    li_col_names = []\n",
    "    for station_name in li_station_names:\n",
    "        li_col_names.append(list(dict_stations[station_name].keys()))\n",
    "    return find_intersection(lists=li_col_names)\n",
    "\n",
    "def select_by(df,y,m=None,d=None):\n",
    "    if m==None and d==None:\n",
    "        return df[df.index.year.isin(y)]\n",
    "    if m==None:\n",
    "        return df[(df.index.year.isin(y))&(df.index.day.isin(d))]\n",
    "    if d==None:\n",
    "        return df[(df.index.month.isin(m))&(df.index.year.isin(y))]\n",
    "    return df[(df.index.day.isin(d))&(df.index.month.isin(m))&(df.index.year.isin(y))]\n",
    "\n",
    "def show_prediction_plot(df_y, dict_predictions=None, li_specific_cols = None, title=\"\"):\n",
    "    df_result = pd.DataFrame()\n",
    "    df_result = df_y.copy()\n",
    "\n",
    "    if dict_predictions!=None:\n",
    "        for key in dict_predictions:\n",
    "            df_result[key] = dict_predictions[key]\n",
    "    if li_specific_cols != None:\n",
    "        df_result = df_result [li_specific_cols]\n",
    "    return model_fun.create_prediction_plot(df=df_result,add_dots=True,period=\"M\",title=title)\n",
    "def save_dict_to_json(dictionary, file_path):\n",
    "    try:\n",
    "        with open(file_path, 'a+') as file:\n",
    "            file.seek(0)\n",
    "            data = file.read()\n",
    "            if data:\n",
    "                file.seek(0, 2)  # Move the cursor to the end of the file\n",
    "                file.write(',')\n",
    "            json.dump(dictionary, file)\n",
    "            file.write('\\n')\n",
    "    except IOError as e:\n",
    "        print(f\"Error writing to JSON file: {str(e)}\")\n",
    "def saveDF(df, exp_desc, filename=\"UNDEFINED\"):\n",
    "    dict_exp_desc = dict()\n",
    "    current_timestamp = int(time.time())\n",
    "    exp_path = Path('../datasets/experiments_results/'+str(current_timestamp)+'_'+filename+'.csv')\n",
    "    df.to_csv(exp_path)\n",
    "    dict_exp_desc[str(current_timestamp)+'_'+filename+'.csv'] = exp_desc\n",
    "    file_path = \"../datasets/experiments_results/exp_description.json\"\n",
    "    save_dict_to_json(dict_exp_desc,file_path)\n",
    "    #with open(file_path, \"a\") as json_file:\n",
    "    #    json.dump(dict_exp_desc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data per measurement station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_DATA = '../datasets/data_per_station'\n",
    "\n",
    "\n",
    "DICT_DF_STATIONS = {'d':pd.DataFrame(),'w':pd.DataFrame(),'s':pd.DataFrame(),'n':pd.DataFrame(),'e':pd.DataFrame(),'z':pd.DataFrame()}\n",
    "DICT_DF_STATIONS_ID = {'d':1,'w':2,'s':3,'n':4,'e':5,'z':6}\n",
    "DICT_DF_STATIONS = model_fun.read_all(path=PATH_DATA,DICT_DF_STATIONS=DICT_DF_STATIONS,DICT_DF_STATIONS_ID=DICT_DF_STATIONS_ID,use_lags=True)\n",
    "ALL_POLLUTANTS = [\"pm10\",\"nox\",\"no\",\"no2\",\"pm2.5\",\"pm1\",\"o3\"]\n",
    "\n",
    "# drop features with a high number of NaN which are non-imputeable\n",
    "\n",
    "if \"windsp\" in DICT_DF_STATIONS[\"e\"].columns:\n",
    "    DICT_DF_STATIONS[\"e\"].drop(columns=[\"windDirDeg\",\"windsp\",\"windPeak\",\"windDirClass\"],inplace=True)\n",
    "if \"radiation\" in DICT_DF_STATIONS[\"e\"].columns:\n",
    "    DICT_DF_STATIONS[\"n\"].drop(columns=[\"radiation\"],inplace=True)\n",
    "\n",
    "\n",
    "DICT_DF_STATIONS = model_fun.use_traffic_data(dict_stations=DICT_DF_STATIONS, use_traffic_bins=False, use_traffic_continous=False)\n",
    "DICT_DF_STATIONS = model_fun.use_traffic_lags(dict_stations=DICT_DF_STATIONS, only_use_lags=False, use_traffic_lags=False)\n",
    "\n",
    "# Drop few NaN values found in pre-processing\n",
    "for station in DICT_DF_STATIONS:\n",
    "    DICT_DF_STATIONS[station].dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select Features according to EDA\n",
    "- to get a baseline with pm10lags, set use_pm10_lag=True\n",
    "- to encode cyclic nature of dayOfYear in sine, set dayOfYear_sine_transform = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_categorical = True\n",
    "use_degree = False\n",
    "use_zagreb_year = False\n",
    "\n",
    "\n",
    "use_traffic_continous = False\n",
    "use_traffic_bins = False\n",
    "\n",
    "\n",
    "use_traffic_lags = False\n",
    "only_use_lags = False\n",
    "\n",
    "use_pm10_lags = False\n",
    "\n",
    "delete_holiday_features = False\n",
    "\n",
    "# exclude 2020\n",
    "exclude_year = None\n",
    "\n",
    "exclude_years_zagreb = True\n",
    "\n",
    "dayOfYear_sine_transform = False\n",
    "\n",
    "###################################################################################################################################################\n",
    "\n",
    "if exclude_year!= None:\n",
    "    for station in DICT_DF_STATIONS:\n",
    "        df_temp = DICT_DF_STATIONS[station]\n",
    "        DICT_DF_STATIONS[station] = df_temp[df_temp.index.year != exclude_year]\n",
    "        print(set(DICT_DF_STATIONS[station].index.year))\n",
    "        \n",
    "# to exlude 2009,2010,2011,2012,2013\n",
    "if exclude_years_zagreb:\n",
    "    for year in [2009,2010,2011,2012,2013]:\n",
    "        df_temp = DICT_DF_STATIONS[\"z\"]\n",
    "        DICT_DF_STATIONS[\"z\"] = df_temp[df_temp.index.year != year]\n",
    "        print(\"Station Zagreb: delete year: \"+ str(year))\n",
    "\n",
    "if not use_categorical:\n",
    "    for station in DICT_DF_STATIONS:\n",
    "        if \"windDirClass\" in DICT_DF_STATIONS[station].columns:\n",
    "            DICT_DF_STATIONS[station].drop('windDirClass',axis=1,inplace=True)\n",
    "            print(\"DELETE [windDirClass] done for station '\"+fun.get_station_name_by_indice(station)+\"'\")\n",
    "\n",
    "if not use_categorical:\n",
    "    for station in DICT_DF_STATIONS:\n",
    "        if \"windDirClass\" in DICT_DF_STATIONS[station].columns:\n",
    "            DICT_DF_STATIONS[station].drop('windDirClass',axis=1,inplace=True)\n",
    "            print(\"DELETE [windDirClass] done for station '\"+fun.get_station_name_by_indice(station)+\"'\")\n",
    "if not use_degree:\n",
    "    for station in DICT_DF_STATIONS:\n",
    "        if \"windDirDeg\" in DICT_DF_STATIONS[station].columns:\n",
    "            DICT_DF_STATIONS[station] = DICT_DF_STATIONS[station].drop('windDirDeg',axis=1)\n",
    "            print(\"DELETE [windDirDeg] done for station '\"+fun.get_station_name_by_indice(station)+\"'\")\n",
    "            \n",
    "if not use_zagreb_year:\n",
    "    if \"year\" in DICT_DF_STATIONS[\"z\"].columns:\n",
    "            DICT_DF_STATIONS[\"z\"] = DICT_DF_STATIONS[\"z\"].drop('year',axis=1)\n",
    "            print(\"DELETE [year] done for station '\"+fun.get_station_name_by_indice(\"z\")+\"'\")\n",
    "\n",
    "if not use_traffic_continous:\n",
    "    for station in DICT_DF_STATIONS:\n",
    "        if \"traffic\" in DICT_DF_STATIONS[station].columns:\n",
    "            DICT_DF_STATIONS[station].drop('traffic',axis=1,inplace=True)\n",
    "            print(\"DELETE [traffic] done for station '\"+fun.get_station_name_by_indice(station)+\"'\")\n",
    "if not use_traffic_bins:\n",
    "    for station in DICT_DF_STATIONS:\n",
    "        if \"trafficClass\" in DICT_DF_STATIONS[station].columns:\n",
    "            DICT_DF_STATIONS[station].drop('trafficClass',axis=1,inplace=True)\n",
    "            print(\"DELETE [trafficClass] done for station '\"+fun.get_station_name_by_indice(station)+\"'\")\n",
    "            \n",
    "if only_use_lags:\n",
    "    for station in DICT_DF_STATIONS:\n",
    "        if \"traffic\" in DICT_DF_STATIONS[station].columns:\n",
    "            DICT_DF_STATIONS[station].drop('traffic',axis=1,inplace=True)\n",
    "            print(\"DELETE [traffic] done for station '\"+ fun.get_station_name_by_indice(station)+\"'\")\n",
    "# drop lag columns\n",
    "if not use_traffic_lags:\n",
    "    for station in DICT_DF_STATIONS:\n",
    "        li_lags = [\"trafficLag1\",\"trafficLag2\",\"trafficLag3\",\"trafficLag4\"]\n",
    "        for traffic_lag in li_lags:\n",
    "            if traffic_lag in DICT_DF_STATIONS[station].columns:\n",
    "                DICT_DF_STATIONS[station].drop(traffic_lag ,axis=1,inplace=True)\n",
    "                print(\"DELETE\"+ traffic_lag + \"done for station '\"+fun.get_station_name_by_indice(station)+\"'\")\n",
    "                \n",
    "# drop lag columns\n",
    "if not use_pm10_lags:\n",
    "    for station in DICT_DF_STATIONS:\n",
    "        if \"pm10Lag\" in DICT_DF_STATIONS[station].columns:\n",
    "            DICT_DF_STATIONS[station].drop('pm10Lag',axis=1,inplace=True)\n",
    "            print(\"DELETE done for station '\"+fun.get_station_name_by_indice(station)+\"'\")\n",
    "            \n",
    "if delete_holiday_features:\n",
    "    for station in DICT_DF_STATIONS:\n",
    "        if \"holiday\" in DICT_DF_STATIONS[station].columns:\n",
    "            DICT_DF_STATIONS[station].drop('holiday',axis=1,inplace=True)\n",
    "            print(\"DELETE [holiday] done for station '\"+fun.get_station_name_by_indice(station)+\"'\")\n",
    "        if \"dayBeforeHoliday\" in DICT_DF_STATIONS[station].columns:\n",
    "            DICT_DF_STATIONS[station].drop('dayBeforeHoliday',axis=1,inplace=True)\n",
    "            print(\"DELETE [dayBeforeHoliday] done for station '\"+fun.get_station_name_by_indice(station)+\"'\")\n",
    "        if \"dayAfterHoliday\" in DICT_DF_STATIONS[station].columns:\n",
    "            DICT_DF_STATIONS[station].drop('dayAfterHoliday',axis=1,inplace=True)\n",
    "            print(\"DELETE [dayAfterHoliday] done for station '\"+fun.get_station_name_by_indice(station)+\"'\")\n",
    "\n",
    "# Function to get the day of the year dynamically considering leap years\n",
    "def get_day_of_year_adjusted(date):\n",
    "    year = date.year\n",
    "    days_in_year = 366 if calendar.isleap(year) else 365\n",
    "    return date.timetuple().tm_yday, days_in_year\n",
    "\n",
    "if dayOfYear_sine_transform:\n",
    "      for station in DICT_DF_STATIONS:\n",
    "        DICT_DF_STATIONS[station][\"dayOfYear\"] = [\n",
    "            np.sin(2 * np.pi * get_day_of_year_adjusted(date)[0] / get_day_of_year_adjusted(date)[1])\n",
    "            for date in DICT_DF_STATIONS[station].index.to_pydatetime()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impute RH and Temp with TL from stations West,North,DB,South to station East\n",
    "- only done if new_TL_imputations_required = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_TL_imputations_required = False\n",
    "\n",
    "if new_TL_imputations_required:\n",
    "    DICT_METRICS = {}\n",
    "\n",
    "\n",
    "    li_all_station_graz = ['w', 'n', 's' ,'e']\n",
    "        \n",
    "    dict_predictions = {}\n",
    "\n",
    "    liFeaturesToImpute = [\"temp\",\"rh\"]\n",
    "\n",
    "    li_source = [x for x in li_all_station_graz if x!=\"z\"]\n",
    "    target =  \"z\"\n",
    "\n",
    "    print(str([fun.get_station_name_by_indice(station) for station in li_source]) +\" -> \"+fun.get_station_name_by_indice(target))\n",
    "\n",
    "    for featureToImpute in liFeaturesToImpute:\n",
    "\n",
    "        df_global = model_fun.create_global_model(dict_stations=DICT_DF_STATIONS,li_station_combination=li_all_station_graz)\n",
    "\n",
    "        df_s_train_X, df_s_train_y, df_t_test_X, df_t_test_y, _, _, dict_info = model_fun.split_train_test_by_station(df=df_global,\n",
    "                                                                                                                                li_stations=li_all_station_graz,\n",
    "                                                                                                                                pollutant_to_predict=\"pm10\",\n",
    "                                                                                                                                station_test=\"e\",\n",
    "                                                                                                                                station_validation=None,\n",
    "                                                                                                                                use_validation = False,\n",
    "                                                                                                                                output=True)\n",
    "\n",
    "        X_TRAIN = df_s_train_X.loc[:, df_s_train_X.columns != 'temp']\n",
    "        X_TRAIN = X_TRAIN.loc[:, X_TRAIN.columns != 'rh']\n",
    "        X_TRAIN[\"pm10\"] = df_s_train_y\n",
    "\n",
    "        X_TRAIN\n",
    "\n",
    "        Y_TRAIN = df_s_train_X.loc[:, df_s_train_X.columns == featureToImpute]\n",
    "\n",
    "        X_TEST = df_t_test_X.loc[:, df_t_test_X.columns != 'temp']\n",
    "        X_TEST = X_TEST.loc[:, X_TEST.columns != 'rh']\n",
    "        X_TEST[\"pm10\"] = df_t_test_y\n",
    "\n",
    "        Y_TEST = df_t_test_X.loc[:, df_t_test_X.columns == featureToImpute]\n",
    "\n",
    "        years = [2018,2019,2020]\n",
    "\n",
    "\n",
    "        yt_samples_for_TL = select_by(df=Y_TEST,y=years)\n",
    "        Xt_samples_for_TL = select_by(df=X_TEST,y=years)\n",
    "\n",
    "        print(\"Years\",str(years))\n",
    "        print(\"No. of injections: \", len(Xt_samples_for_TL))\n",
    "        no_injections = len(Xt_samples_for_TL)\n",
    "\n",
    "        # Random Forest\n",
    "        regr = RandomForestRegressor(random_state=42,n_estimators=180,min_samples_split=5, min_samples_leaf=2, max_features=\"sqrt\", max_depth=60, bootstrap=True)\n",
    "        regr.fit(X=X_TRAIN, y=Y_TRAIN.values.ravel())\n",
    "        predictions_RF = regr.predict(X_TEST)\n",
    "\n",
    "        # Random Forest with TrAdaBoostR2\n",
    "        model = TrAdaBoostR2(regr, Xt=Xt_samples_for_TL, yt=yt_samples_for_TL,n_estimators=20, random_state=0,verbose=0,lr=2)\n",
    "        model.fit(X_TRAIN, Y_TRAIN)\n",
    "        predictions_RF_TL = model.predict(X_TEST)\n",
    "\n",
    "\n",
    "\n",
    "        #dict_predictions[\"Oo_DG_\"+featureToImpute] = predictions_RF\n",
    "        dict_predictions[\"TrABR2_\"+featureToImpute] = predictions_RF_TL.flatten()\n",
    "\n",
    "\n",
    "\n",
    "    # only uncomment if new TL computations are required! \n",
    "    #df = pd.DataFrame(data=dict_predictions,index=X_TEST.index)\n",
    "    #df.to_csv(\"../datasets/MissingValImputeTL.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read TL imputed data and combine with dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_DATA = '../datasets/data_per_station_TL_impute_station_east'\n",
    "\n",
    "\n",
    "DICT_DF_STATIONS = {'d':pd.DataFrame(),'w':pd.DataFrame(),'s':pd.DataFrame(),'n':pd.DataFrame(),'e':pd.DataFrame(),'z':pd.DataFrame()}\n",
    "DICT_DF_STATIONS_ID = {'d':1,'w':2,'s':3,'n':4,'e':5,'z':6}\n",
    "DICT_DF_STATIONS = model_fun.read_all(path=PATH_DATA,DICT_DF_STATIONS=DICT_DF_STATIONS,DICT_DF_STATIONS_ID=DICT_DF_STATIONS_ID,use_lags=True)\n",
    "ALL_POLLUTANTS = [\"pm10\",\"nox\",\"no\",\"no2\",\"pm2.5\",\"pm1\",\"o3\"]\n",
    "\n",
    "# drop features with a high number of NaN which are non-imputeable\n",
    "\n",
    "if \"windsp\" in DICT_DF_STATIONS[\"e\"].columns:\n",
    "    DICT_DF_STATIONS[\"e\"].drop(columns=[\"windDirDeg\",\"windsp\",\"windPeak\",\"windDirClass\"],inplace=True)\n",
    "if \"radiation\" in DICT_DF_STATIONS[\"e\"].columns:\n",
    "    DICT_DF_STATIONS[\"n\"].drop(columns=[\"radiation\"],inplace=True)\n",
    "\n",
    "\n",
    "dfTLImputed_RH_temp = pd.read_csv(\"../datasets/MissingValImputeTL.csv\",index_col=0, parse_dates=True)\n",
    "\n",
    "\n",
    "def set_value_if_none(row):\n",
    "    if row[1] == -99:\n",
    "        return row[0]\n",
    "    else:\n",
    "        return row[1]\n",
    "    \n",
    "\n",
    "DICT_DF_STATIONS[\"e\"] = DICT_DF_STATIONS[\"e\"].combine_first(dfTLImputed_RH_temp)\n",
    "\n",
    "\n",
    "DICT_DF_STATIONS['e'][\"temp\"] = DICT_DF_STATIONS['e'][[\"TrABR2_temp\",\"temp\"]].apply(set_value_if_none,axis=1)\n",
    "DICT_DF_STATIONS['e'][\"rh\"] = DICT_DF_STATIONS['e'][[\"TrABR2_temp\",\"rh\"]].apply(set_value_if_none,axis=1)\n",
    "\n",
    "\n",
    "DICT_DF_STATIONS['e'].drop(columns=[\"TrABR2_rh\",\"TrABR2_temp\"],inplace=True)\n",
    "\n",
    "\n",
    "# Drop few NaN values found in pre-processing\n",
    "for station in DICT_DF_STATIONS:\n",
    "    DICT_DF_STATIONS[station].dropna(inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### kdkdkd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_categorical = True\n",
    "use_degree = False\n",
    "use_zagreb_year = False\n",
    "\n",
    "\n",
    "use_traffic_continous = False\n",
    "use_traffic_bins = False\n",
    "\n",
    "\n",
    "use_traffic_lags = False\n",
    "only_use_lags = False\n",
    "\n",
    "use_pm10_lags = False\n",
    "\n",
    "delete_holiday_features = False\n",
    "\n",
    "# exclude 2020\n",
    "exclude_year = None\n",
    "\n",
    "exclude_years_zagreb = True\n",
    "\n",
    "dayOfYear_sine_transform = False\n",
    "\n",
    "###################################################################################################################################################\n",
    "\n",
    "if exclude_year!= None:\n",
    "    for station in DICT_DF_STATIONS:\n",
    "        df_temp = DICT_DF_STATIONS[station]\n",
    "        DICT_DF_STATIONS[station] = df_temp[df_temp.index.year != exclude_year]\n",
    "        print(set(DICT_DF_STATIONS[station].index.year))\n",
    "        \n",
    "# to exlude 2009,2010,2011,2012,2013\n",
    "if exclude_years_zagreb:\n",
    "    for year in [2009,2010,2011,2012,2013]:\n",
    "        df_temp = DICT_DF_STATIONS[\"z\"]\n",
    "        DICT_DF_STATIONS[\"z\"] = df_temp[df_temp.index.year != year]\n",
    "        print(\"Station Zagreb: delete year: \"+ str(year))\n",
    "\n",
    "if not use_categorical:\n",
    "    for station in DICT_DF_STATIONS:\n",
    "        if \"windDirClass\" in DICT_DF_STATIONS[station].columns:\n",
    "            DICT_DF_STATIONS[station].drop('windDirClass',axis=1,inplace=True)\n",
    "            print(\"DELETE [windDirClass] done for station '\"+fun.get_station_name_by_indice(station)+\"'\")\n",
    "\n",
    "if not use_categorical:\n",
    "    for station in DICT_DF_STATIONS:\n",
    "        if \"windDirClass\" in DICT_DF_STATIONS[station].columns:\n",
    "            DICT_DF_STATIONS[station].drop('windDirClass',axis=1,inplace=True)\n",
    "            print(\"DELETE [windDirClass] done for station '\"+fun.get_station_name_by_indice(station)+\"'\")\n",
    "if not use_degree:\n",
    "    for station in DICT_DF_STATIONS:\n",
    "        if \"windDirDeg\" in DICT_DF_STATIONS[station].columns:\n",
    "            DICT_DF_STATIONS[station] = DICT_DF_STATIONS[station].drop('windDirDeg',axis=1)\n",
    "            print(\"DELETE [windDirDeg] done for station '\"+fun.get_station_name_by_indice(station)+\"'\")\n",
    "            \n",
    "if not use_zagreb_year:\n",
    "    if \"year\" in DICT_DF_STATIONS[\"z\"].columns:\n",
    "            DICT_DF_STATIONS[\"z\"] = DICT_DF_STATIONS[\"z\"].drop('year',axis=1)\n",
    "            print(\"DELETE [year] done for station '\"+fun.get_station_name_by_indice(\"z\")+\"'\")\n",
    "\n",
    "if not use_traffic_continous:\n",
    "    for station in DICT_DF_STATIONS:\n",
    "        if \"traffic\" in DICT_DF_STATIONS[station].columns:\n",
    "            DICT_DF_STATIONS[station].drop('traffic',axis=1,inplace=True)\n",
    "            print(\"DELETE [traffic] done for station '\"+fun.get_station_name_by_indice(station)+\"'\")\n",
    "if not use_traffic_bins:\n",
    "    for station in DICT_DF_STATIONS:\n",
    "        if \"trafficClass\" in DICT_DF_STATIONS[station].columns:\n",
    "            DICT_DF_STATIONS[station].drop('trafficClass',axis=1,inplace=True)\n",
    "            print(\"DELETE [trafficClass] done for station '\"+fun.get_station_name_by_indice(station)+\"'\")\n",
    "            \n",
    "if only_use_lags:\n",
    "    for station in DICT_DF_STATIONS:\n",
    "        if \"traffic\" in DICT_DF_STATIONS[station].columns:\n",
    "            DICT_DF_STATIONS[station].drop('traffic',axis=1,inplace=True)\n",
    "            print(\"DELETE [traffic] done for station '\"+ fun.get_station_name_by_indice(station)+\"'\")\n",
    "# drop lag columns\n",
    "if not use_traffic_lags:\n",
    "    for station in DICT_DF_STATIONS:\n",
    "        li_lags = [\"trafficLag1\",\"trafficLag2\",\"trafficLag3\",\"trafficLag4\"]\n",
    "        for traffic_lag in li_lags:\n",
    "            if traffic_lag in DICT_DF_STATIONS[station].columns:\n",
    "                DICT_DF_STATIONS[station].drop(traffic_lag ,axis=1,inplace=True)\n",
    "                print(\"DELETE\"+ traffic_lag + \"done for station '\"+fun.get_station_name_by_indice(station)+\"'\")\n",
    "                \n",
    "# drop lag columns\n",
    "if not use_pm10_lags:\n",
    "    for station in DICT_DF_STATIONS:\n",
    "        if \"pm10Lag\" in DICT_DF_STATIONS[station].columns:\n",
    "            DICT_DF_STATIONS[station].drop('pm10Lag',axis=1,inplace=True)\n",
    "            print(\"DELETE done for station '\"+fun.get_station_name_by_indice(station)+\"'\")\n",
    "            \n",
    "if delete_holiday_features:\n",
    "    for station in DICT_DF_STATIONS:\n",
    "        if \"holiday\" in DICT_DF_STATIONS[station].columns:\n",
    "            DICT_DF_STATIONS[station].drop('holiday',axis=1,inplace=True)\n",
    "            print(\"DELETE [holiday] done for station '\"+fun.get_station_name_by_indice(station)+\"'\")\n",
    "        if \"dayBeforeHoliday\" in DICT_DF_STATIONS[station].columns:\n",
    "            DICT_DF_STATIONS[station].drop('dayBeforeHoliday',axis=1,inplace=True)\n",
    "            print(\"DELETE [dayBeforeHoliday] done for station '\"+fun.get_station_name_by_indice(station)+\"'\")\n",
    "        if \"dayAfterHoliday\" in DICT_DF_STATIONS[station].columns:\n",
    "            DICT_DF_STATIONS[station].drop('dayAfterHoliday',axis=1,inplace=True)\n",
    "            print(\"DELETE [dayAfterHoliday] done for station '\"+fun.get_station_name_by_indice(station)+\"'\")\n",
    "\n",
    "# Function to get the day of the year dynamically considering leap years\n",
    "def get_day_of_year_adjusted(date):\n",
    "    year = date.year\n",
    "    days_in_year = 366 if calendar.isleap(year) else 365\n",
    "    return date.timetuple().tm_yday, days_in_year\n",
    "\n",
    "if dayOfYear_sine_transform:\n",
    "      for station in DICT_DF_STATIONS:\n",
    "        DICT_DF_STATIONS[station][\"dayOfYear\"] = [\n",
    "            np.sin(2 * np.pi * get_day_of_year_adjusted(date)[0] / get_day_of_year_adjusted(date)[1])\n",
    "            for date in DICT_DF_STATIONS[station].index.to_pydatetime()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Station level OODG (e->z) using newly imputed missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DICT_METRICS = {}\n",
    "\n",
    "# all station test on Zagreb\n",
    "li_combinations = [[\"e\",\"z\"]]\n",
    "\n",
    "dict_predictions = {}\n",
    "\n",
    "df_results = pd.DataFrame()\n",
    "\n",
    "for combination in li_combinations:\n",
    "   \n",
    "    source = combination[0]\n",
    "    target = combination[1]\n",
    "        \n",
    "    print(fun.get_station_name_by_indice(source)+\" -> \"+fun.get_station_name_by_indice(target))\n",
    "\n",
    "    df_s_train_X, df_s_train_y, _, _, dict_s_info = fun.split_train_test_by_year(df=DICT_DF_STATIONS[source],pollutant_to_predict=\"pm10\",split_ratio=1.0,output=False)\n",
    "    _, _, df_t_test_X, df_t_test_y, dict_t_info = fun.split_train_test_by_year(df=DICT_DF_STATIONS[target],pollutant_to_predict=\"pm10\",split_ratio=1.0,output=False)\n",
    "    \n",
    "    # only consider equal features\n",
    "    li_columns = get_colname_intersection(dict_stations=DICT_DF_STATIONS,li_station_names=[source,target])\n",
    "    \n",
    "    li_columns = [x for x in li_columns if x not in ALL_POLLUTANTS]\n",
    "    df_s_train_X = df_s_train_X[li_columns]\n",
    "    df_t_test_X =  df_t_test_X[li_columns]\n",
    "   \n",
    "    \n",
    "    # Random Forest \n",
    "    regr = RandomForestRegressor(random_state=42,n_estimators=180,min_samples_split=5, min_samples_leaf=2, max_features=\"sqrt\", max_depth=60, bootstrap=True)    \n",
    "    regr.fit(X=df_s_train_X, y=df_s_train_y.values.ravel())\n",
    "    predictions_RF = regr.predict(df_t_test_X)\n",
    "    \n",
    "    calc_kpi(pred_result=predictions_RF,test_X=df_t_test_X,test_y=df_t_test_y,title=\"RF_pred\",output=False)\n",
    "        \n",
    "    dict_predictions[\"(\"+source+\"->\"+target+\")\"] = predictions_RF\n",
    "\n",
    "    df_results[\"(\"+source+\"->\"+target+\")\"] = pd.DataFrame(DICT_METRICS, index=['rmse', 'nrmse', 'mape'])\n",
    "    display(df_results)\n",
    "\n",
    "\n",
    "show_prediction_plot(df_y=df_t_test_y,dict_predictions=dict_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Station-level TL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DICT_METRICS = {}\n",
    "\n",
    "li_combinations = [[\"e\",\"z\"]]\n",
    "\n",
    "for combination in li_combinations:\n",
    "    \n",
    "    \n",
    "    dict_predictions = {}\n",
    "    \n",
    "    source = combination[0]\n",
    "    target = combination[1]\n",
    "        \n",
    "    print(fun.get_station_name_by_indice(source)+\" -> \"+fun.get_station_name_by_indice(target))\n",
    "\n",
    "    df_s_train_X, df_s_train_y, _, _, dict_s_info = fun.split_train_test_by_year(df=DICT_DF_STATIONS[source],pollutant_to_predict=\"pm10\",split_ratio=1.0,output=False)\n",
    "    _, _, df_t_test_X, df_t_test_y, dict_t_info = fun.split_train_test_by_year(df=DICT_DF_STATIONS[target],pollutant_to_predict=\"pm10\",split_ratio=1.0,output=False)\n",
    "    \n",
    "    # only consider equal features\n",
    "    li_columns = get_colname_intersection(dict_stations=DICT_DF_STATIONS,li_station_names=[source,target])\n",
    "    \n",
    "    li_columns = [x for x in li_columns if x not in ALL_POLLUTANTS]\n",
    "    df_s_train_X = df_s_train_X[li_columns]\n",
    "    df_t_test_X =  df_t_test_X[li_columns]\n",
    "   \n",
    "    years = [2014,2015,2016,2017,2018,2019,2020]\n",
    "    months = [1,7]\n",
    "    \n",
    "    yt_samples_for_TL = select_by(df=df_t_test_y,y=years,m=months)\n",
    "    Xt_samples_for_TL = select_by(df=df_t_test_X,y=years,m=months)\n",
    "    \n",
    "    print(\"No. of injections: \", len(Xt_samples_for_TL))\n",
    "    \n",
    "    # Random Forest \n",
    "    regr = RandomForestRegressor(random_state=42,n_estimators=180,min_samples_split=5, min_samples_leaf=2, max_features=\"sqrt\", max_depth=60, bootstrap=True)    \n",
    "    regr.fit(X=df_s_train_X, y=df_s_train_y.values.ravel())\n",
    "    predictions_RF = regr.predict(df_t_test_X)\n",
    "    \n",
    "    # Random Forest with TrAdaBoostR2\n",
    "    model_ada_RF = TrAdaBoostR2(regr, Xt=Xt_samples_for_TL, yt=yt_samples_for_TL,n_estimators=20, random_state=0,verbose=0,lr=1)\n",
    "    model_ada_RF.fit(df_s_train_X, df_s_train_y)\n",
    "    predictions_RF_TL = model_ada_RF.predict(df_t_test_X)\n",
    "    \n",
    "    \n",
    "    \n",
    "    calc_kpi(pred_result=predictions_RF,test_X=df_t_test_X,test_y=df_t_test_y,title=\"RF_pred\",output=False)\n",
    "    calc_kpi(pred_result=predictions_RF_TL,test_X=df_t_test_X,test_y=df_t_test_y,title=\"RF_ada_pred\",output=False)\n",
    "\n",
    "    \n",
    "    dict_predictions[\"OoDG_pred\"] = predictions_RF\n",
    "    dict_predictions[\"TrAdaBR2_pred\"] = predictions_RF_TL\n",
    "  \n",
    "    \n",
    "    \n",
    "    df_results = pd.DataFrame(DICT_METRICS, index=['rmse', 'nrmse', 'mape'])\n",
    "    display(df_results)\n",
    "\n",
    "#dict_predictions\n",
    "\n",
    "show_prediction_plot(df_y=df_t_test_y,dict_predictions=dict_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### City-level OODG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers.legacy import Adam\n",
    "\n",
    "DICT_METRICS = {}\n",
    "\n",
    "li_all_station_graz = [['z','n', 'e', 's', 'w', 'd'],['z','e','s', 'w', 'd'],['z','n','s','w']]\n",
    "#li_all_station_graz = ['z','n','d']\n",
    "df_results = pd.DataFrame()\n",
    "\n",
    "dict_predictions = {}\n",
    "\n",
    "\n",
    "for li_combinations in li_all_station_graz:\n",
    "    \n",
    "\n",
    "    li_source = [x for x in li_combinations if x!=\"z\"]\n",
    "    target =  \"z\"\n",
    "\n",
    "    print(str([fun.get_station_name_by_indice(station) for station in li_source]) +\" -> \"+fun.get_station_name_by_indice(target))\n",
    "\n",
    "\n",
    "    df_global = model_fun.create_global_model(dict_stations=DICT_DF_STATIONS,li_station_combination=li_combinations)\n",
    "\n",
    "    df_s_train_X, df_s_train_y, df_t_test_X, df_t_test_y, _, _, dict_info = model_fun.split_train_test_by_station(df=df_global,\n",
    "                                                                                                                            li_stations=li_combinations,\n",
    "                                                                                                                            pollutant_to_predict=\"pm10\",\n",
    "                                                                                                                            station_test=target,\n",
    "                                                                                                                            station_validation=None,\n",
    "                                                                                                                            use_validation = False,\n",
    "                                                                                                                            output=True)\n",
    "\n",
    "    # Random Forest\n",
    "    regr = RandomForestRegressor(random_state=42,n_estimators=180,min_samples_split=5, min_samples_leaf=2, max_features=\"sqrt\", max_depth=60, bootstrap=True)\n",
    "    regr.fit(X=df_s_train_X, y=df_s_train_y.values.ravel())\n",
    "    predictions_RF = regr.predict(df_t_test_X)\n",
    "\n",
    "\n",
    "    calc_kpi(pred_result=predictions_RF,test_X=df_t_test_X,test_y=df_t_test_y,title=\"RF_pred\",output=False)\n",
    "\n",
    "    strUniqueKey = str(li_source) +\" -> \"+target\n",
    "    dict_predictions[strUniqueKey] = predictions_RF\n",
    "\n",
    "    df_results[strUniqueKey] = pd.DataFrame(DICT_METRICS, index=['rmse', 'nrmse', 'mape'])\n",
    "    display(df_results)\n",
    "\n",
    "    \n",
    "\n",
    "show_prediction_plot(df_y=df_t_test_y,dict_predictions=dict_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### City-level TL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers.legacy import Adam\n",
    "\n",
    "DICT_METRICS = {}\n",
    "\n",
    "\n",
    "li_all_station_graz = ['z','w', 'n', 's' ,'e','d']\n",
    "    \n",
    "dict_predictions = {}\n",
    "\n",
    "\n",
    "li_source = [x for x in li_all_station_graz if x!=\"z\"]\n",
    "target =  \"z\"\n",
    "\n",
    "print(str([fun.get_station_name_by_indice(station) for station in li_source]) +\" -> \"+fun.get_station_name_by_indice(target))\n",
    "\n",
    "\n",
    "df_global = model_fun.create_global_model(dict_stations=DICT_DF_STATIONS,li_station_combination=li_all_station_graz)\n",
    "df_s_train_X, df_s_train_y, df_t_test_X, df_t_test_y, _, _, dict_info = model_fun.split_train_test_by_station(df=df_global,\n",
    "                                                                                                                        li_stations=li_all_station_graz,\n",
    "                                                                                                                        pollutant_to_predict=\"pm10\",\n",
    "                                                                                                                        station_test=target,\n",
    "                                                                                                                        station_validation=None,\n",
    "                                                                                                                        use_validation = False,\n",
    "                                                                                                                        output=True)\n",
    "\n",
    "\n",
    "years = [2014,2015,2016,2017,2018,2019,2020]\n",
    "months = [1,7]\n",
    "\n",
    "yt_samples_for_TL = select_by(df=df_t_test_y,y=years,m=months)\n",
    "Xt_samples_for_TL = select_by(df=df_t_test_X,y=years,m=months)\n",
    "\n",
    "print(\"Years\",str(years))\n",
    "print(\"No. of injections: \", len(Xt_samples_for_TL))\n",
    "no_injections = len(Xt_samples_for_TL)\n",
    "\n",
    "# Random Forest\n",
    "regr = RandomForestRegressor(random_state=42,n_estimators=180,min_samples_split=5, min_samples_leaf=2, max_features=\"sqrt\", max_depth=60, bootstrap=True)\n",
    "regr.fit(X=df_s_train_X, y=df_s_train_y.values.ravel())\n",
    "predictions_RF = regr.predict(df_t_test_X)\n",
    "\n",
    "\n",
    "# Random Forest with TrAdaBoostR2\n",
    "model = TrAdaBoostR2(regr, Xt=Xt_samples_for_TL, yt=yt_samples_for_TL,n_estimators=20, random_state=0,verbose=0,lr=2)\n",
    "model.fit(df_s_train_X, df_s_train_y)\n",
    "predictions_RF_TL = model.predict(df_t_test_X)\n",
    "\n",
    "\n",
    "\n",
    "calc_kpi(pred_result=predictions_RF,test_X=df_t_test_X,test_y=df_t_test_y,title=\"RF_pred\",output=False)\n",
    "\n",
    "calc_kpi(pred_result=predictions_RF_TL,test_X=df_t_test_X,test_y=df_t_test_y,title=\"RF_ada_pred\",output=False)\n",
    "\n",
    "\n",
    "dict_predictions[\"Oo_DG\"] = predictions_RF\n",
    "dict_predictions[\"TrABR2\"] = predictions_RF_TL\n",
    "# dict_predictions[\"CORAL_pred\"] = predictions_coral_RF\n",
    "# dict_predictions[\"NNW_pred\"] = predictions_NN_RF\n",
    "# dict_predictions[\"KLIEP_pred\"] = predictions_kliep_RF\n",
    "\n",
    "\n",
    "\n",
    "df_results = pd.DataFrame(DICT_METRICS, index=['rmse', 'nrmse', 'mape'])\n",
    "display(df_results)\n",
    "\n",
    "\n",
    "\n",
    "#show_prediction_plot(df_y=df_t_test_y,dict_predictions=dict_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PMForecastingVenV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
