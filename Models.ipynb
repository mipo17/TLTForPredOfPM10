{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import MaxPooling1D,Flatten, Dense, LSTM, Conv1D, TimeDistributed, GRU\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import pandas as pd\n",
    "import model_fun\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "#https://machinelearningmastery.com/cnn-long-short-term-memory-networks/\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "import tensorflow as tf\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import itertools\n",
    "\n",
    "import model_fun as fun\n",
    "\n",
    "from adapt.feature_based import CORAL\n",
    "from adapt.instance_based import KLIEP\n",
    "from adapt.instance_based import KMM\n",
    "from adapt.instance_based import TrAdaBoostR2\n",
    "from adapt.instance_based import NearestNeighborsWeighting\n",
    "from adapt.feature_based import FA\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "import time\n",
    "import json\n",
    "\n",
    "import calendar\n",
    "\n",
    "!python --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_DATA = '../datasets/data_per_station'\n",
    "\n",
    "#PATH_DATA =\"../datasets/data_per_station_OLD\" # mean value imputation\n",
    "# to test how the models behave when using 60% more data\n",
    "#PATH_DATA = '../datasets/data_per_station_2009-2022'\n",
    "\n",
    "DICT_DF_STATIONS = {'d':pd.DataFrame(),'w':pd.DataFrame(),'s':pd.DataFrame(),'n':pd.DataFrame(),'e':pd.DataFrame(),'z':pd.DataFrame()}\n",
    "DICT_DF_STATIONS_ID = {'d':1,'w':2,'s':3,'n':4,'e':5,'z':6}\n",
    "DICT_DF_STATIONS = model_fun.read_all(path=PATH_DATA,DICT_DF_STATIONS=DICT_DF_STATIONS,DICT_DF_STATIONS_ID=DICT_DF_STATIONS_ID,use_lags=True)\n",
    "ALL_POLLUTANTS = [\"pm10\",\"nox\",\"no\",\"no2\",\"pm2.5\",\"pm1\",\"o3\"]\n",
    "\n",
    "# drop features with a high number of NaN which are non-imputeable\n",
    "\n",
    "if \"windsp\" in DICT_DF_STATIONS[\"e\"].columns:\n",
    "    DICT_DF_STATIONS[\"e\"].drop(columns=[\"windDirDeg\",\"windsp\",\"windPeak\",\"windDirClass\"],inplace=True)\n",
    "if \"radiation\" in DICT_DF_STATIONS[\"e\"].columns:\n",
    "    DICT_DF_STATIONS[\"n\"].drop(columns=[\"radiation\"],inplace=True)\n",
    "\n",
    "\n",
    "DICT_DF_STATIONS = model_fun.use_traffic_data(dict_stations=DICT_DF_STATIONS, use_traffic_bins=False, use_traffic_continous=False)\n",
    "DICT_DF_STATIONS = model_fun.use_traffic_lags(dict_stations=DICT_DF_STATIONS, only_use_lags=False, use_traffic_lags=False)\n",
    "\n",
    "# Drop few NaN values found in pre-processing\n",
    "for station in DICT_DF_STATIONS:\n",
    "    DICT_DF_STATIONS[station].dropna(inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select Features according to EDA\n",
    "- to get a baseline with pm10lags, set use_pm10_lag=True\n",
    "- to encode cyclic nature of dayOfYear in sine, set dayOfYear_sine_transform = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_categorical = True\n",
    "use_degree = False\n",
    "use_zagreb_year = False\n",
    "\n",
    "\n",
    "use_traffic_continous = False\n",
    "use_traffic_bins = False\n",
    "\n",
    "\n",
    "use_traffic_lags = False\n",
    "only_use_lags = False\n",
    "\n",
    "use_pm10_lags = False\n",
    "\n",
    "delete_holiday_features = False\n",
    "\n",
    "# exclude 2020\n",
    "exclude_year = None\n",
    "\n",
    "exclude_years_zagreb = True\n",
    "\n",
    "dayOfYear_sine_transform = False\n",
    "\n",
    "###################################################################################################################################################\n",
    "\n",
    "if exclude_year!= None:\n",
    "    for station in DICT_DF_STATIONS:\n",
    "        df_temp = DICT_DF_STATIONS[station]\n",
    "        DICT_DF_STATIONS[station] = df_temp[df_temp.index.year != exclude_year]\n",
    "        print(set(DICT_DF_STATIONS[station].index.year))\n",
    "        \n",
    "# to exlude 2009,2010,2011,2012,2013\n",
    "if exclude_years_zagreb:\n",
    "    for year in [2009,2010,2011,2012,2013]:\n",
    "        df_temp = DICT_DF_STATIONS[\"z\"]\n",
    "        DICT_DF_STATIONS[\"z\"] = df_temp[df_temp.index.year != year]\n",
    "        print(\"Station Zagreb: delete year: \"+ str(year))\n",
    "\n",
    "\n",
    "if not use_categorical:\n",
    "    for station in DICT_DF_STATIONS:\n",
    "        if \"windDirClass\" in DICT_DF_STATIONS[station].columns:\n",
    "            DICT_DF_STATIONS[station].drop('windDirClass',axis=1,inplace=True)\n",
    "            print(\"DELETE [windDirClass] done for station '\"+fun.get_station_name_by_indice(station)+\"'\")\n",
    "\n",
    "if not use_categorical:\n",
    "    for station in DICT_DF_STATIONS:\n",
    "        if \"windDirClass\" in DICT_DF_STATIONS[station].columns:\n",
    "            DICT_DF_STATIONS[station].drop('windDirClass',axis=1,inplace=True)\n",
    "            print(\"DELETE [windDirClass] done for station '\"+fun.get_station_name_by_indice(station)+\"'\")\n",
    "if not use_degree:\n",
    "    for station in DICT_DF_STATIONS:\n",
    "        if \"windDirDeg\" in DICT_DF_STATIONS[station].columns:\n",
    "            DICT_DF_STATIONS[station] = DICT_DF_STATIONS[station].drop('windDirDeg',axis=1)\n",
    "            print(\"DELETE [windDirDeg] done for station '\"+fun.get_station_name_by_indice(station)+\"'\")\n",
    "            \n",
    "\n",
    "if not use_zagreb_year:\n",
    "    if \"year\" in DICT_DF_STATIONS[\"z\"].columns:\n",
    "            DICT_DF_STATIONS[\"z\"] = DICT_DF_STATIONS[\"z\"].drop('year',axis=1)\n",
    "            print(\"DELETE [year] done for station '\"+fun.get_station_name_by_indice(\"z\")+\"'\")\n",
    "\n",
    "if not use_traffic_continous:\n",
    "    for station in DICT_DF_STATIONS:\n",
    "        if \"traffic\" in DICT_DF_STATIONS[station].columns:\n",
    "            DICT_DF_STATIONS[station].drop('traffic',axis=1,inplace=True)\n",
    "            print(\"DELETE [traffic] done for station '\"+fun.get_station_name_by_indice(station)+\"'\")\n",
    "if not use_traffic_bins:\n",
    "    for station in DICT_DF_STATIONS:\n",
    "        if \"trafficClass\" in DICT_DF_STATIONS[station].columns:\n",
    "            DICT_DF_STATIONS[station].drop('trafficClass',axis=1,inplace=True)\n",
    "            print(\"DELETE [trafficClass] done for station '\"+fun.get_station_name_by_indice(station)+\"'\")\n",
    "            \n",
    "\n",
    "if only_use_lags:\n",
    "    for station in DICT_DF_STATIONS:\n",
    "        if \"traffic\" in DICT_DF_STATIONS[station].columns:\n",
    "            DICT_DF_STATIONS[station].drop('traffic',axis=1,inplace=True)\n",
    "            print(\"DELETE [traffic] done for station '\"+ fun.get_station_name_by_indice(station)+\"'\")\n",
    "# drop lag columns\n",
    "if not use_traffic_lags:\n",
    "    for station in DICT_DF_STATIONS:\n",
    "        li_lags = [\"trafficLag1\",\"trafficLag2\",\"trafficLag3\",\"trafficLag4\"]\n",
    "        for traffic_lag in li_lags:\n",
    "            if traffic_lag in DICT_DF_STATIONS[station].columns:\n",
    "                DICT_DF_STATIONS[station].drop(traffic_lag ,axis=1,inplace=True)\n",
    "                print(\"DELETE\"+ traffic_lag + \"done for station '\"+fun.get_station_name_by_indice(station)+\"'\")\n",
    "                \n",
    "\n",
    "# drop lag columns\n",
    "if not use_pm10_lags:\n",
    "    for station in DICT_DF_STATIONS:\n",
    "        if \"pm10Lag\" in DICT_DF_STATIONS[station].columns:\n",
    "            DICT_DF_STATIONS[station].drop('pm10Lag',axis=1,inplace=True)\n",
    "            print(\"DELETE done for station '\"+fun.get_station_name_by_indice(station)+\"'\")\n",
    "            \n",
    "if delete_holiday_features:\n",
    "    for station in DICT_DF_STATIONS:\n",
    "        if \"holiday\" in DICT_DF_STATIONS[station].columns:\n",
    "            DICT_DF_STATIONS[station].drop('holiday',axis=1,inplace=True)\n",
    "            print(\"DELETE [holiday] done for station '\"+fun.get_station_name_by_indice(station)+\"'\")\n",
    "        if \"dayBeforeHoliday\" in DICT_DF_STATIONS[station].columns:\n",
    "            DICT_DF_STATIONS[station].drop('dayBeforeHoliday',axis=1,inplace=True)\n",
    "            print(\"DELETE [dayBeforeHoliday] done for station '\"+fun.get_station_name_by_indice(station)+\"'\")\n",
    "        if \"dayAfterHoliday\" in DICT_DF_STATIONS[station].columns:\n",
    "            DICT_DF_STATIONS[station].drop('dayAfterHoliday',axis=1,inplace=True)\n",
    "            print(\"DELETE [dayAfterHoliday] done for station '\"+fun.get_station_name_by_indice(station)+\"'\")\n",
    "\n",
    "# Function to get the day of the year dynamically considering leap years\n",
    "def get_day_of_year_adjusted(date):\n",
    "    year = date.year\n",
    "    days_in_year = 366 if calendar.isleap(year) else 365\n",
    "    return date.timetuple().tm_yday, days_in_year\n",
    "\n",
    "if dayOfYear_sine_transform:\n",
    "      for station in DICT_DF_STATIONS:\n",
    "        DICT_DF_STATIONS[station][\"dayOfYear\"] = [\n",
    "            np.sin(2 * np.pi * get_day_of_year_adjusted(date)[0] / get_day_of_year_adjusted(date)[1])\n",
    "            for date in DICT_DF_STATIONS[station].index.to_pydatetime()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requried Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_kpi(test_X,test_y,pred_result, title=\"NOTDEFINED\",additional_info=None,output=False):\n",
    "    rmse = float(format(np.sqrt(mean_squared_error(test_y, pred_result)), '.3f'))\n",
    "    mape = float(format(mean_absolute_percentage_error(test_y, pred_result), '.3f'))\n",
    "    max_min = test_y.max()-test_y.min()\n",
    "    nrmse = float(format(rmse / (max_min[0])*100, '.3f'))\n",
    "    if output:\n",
    "        print(\"-----------------------------------------\")\n",
    "        print(title)\n",
    "        print(\"RMSE:\\t\\t\\t\\t\"+ str(rmse))\n",
    "        print(\"NRMSE:\\t\\t\\t\\t\"+ str(nrmse))\n",
    "        print(\"MAPE:\\t\\t\\t\\t\"+ str(mape))\n",
    "        print(\"-----------------------------------------\")\n",
    "    \n",
    "    DICT_METRICS[title] = (rmse,nrmse,mape)\n",
    "    \n",
    "    if additional_info!=None:\n",
    "        DICT_METRICS[title] = (rmse,nrmse,mape)+additional_info\n",
    "    \n",
    "# functions needed to calculate similiar features (only select same features)\n",
    "def find_intersection(lists)-> list:\n",
    "    intersection = set(lists[0])\n",
    "    for li in lists[1:]:\n",
    "        intersection = intersection.intersection(li)\n",
    "    return list(intersection)\n",
    "\n",
    "def get_colname_intersection(dict_stations, li_station_names):\n",
    "    li_col_names = []\n",
    "    for station_name in li_station_names:\n",
    "        li_col_names.append(list(dict_stations[station_name].keys()))\n",
    "    return find_intersection(lists=li_col_names)\n",
    "\n",
    "def select_by(df,y,m=None,d=None):\n",
    "    if m==None and d==None:\n",
    "        return df[df.index.year.isin(y)]\n",
    "    if m==None:\n",
    "        return df[(df.index.year.isin(y))&(df.index.day.isin(d))]\n",
    "    if d==None:\n",
    "        return df[(df.index.month.isin(m))&(df.index.year.isin(y))]\n",
    "    return df[(df.index.day.isin(d))&(df.index.month.isin(m))&(df.index.year.isin(y))]\n",
    "def show_prediction_plot(df_y, dict_predictions=None, li_specific_cols = None, title=\"\"):\n",
    "    df_result = pd.DataFrame()\n",
    "    df_result = df_y.copy()\n",
    "\n",
    "    if dict_predictions!=None:\n",
    "        for key in dict_predictions:\n",
    "            df_result[key] = dict_predictions[key]\n",
    "    if li_specific_cols != None:\n",
    "        df_result = df_result [li_specific_cols]\n",
    "    return model_fun.create_prediction_plot(df=df_result,add_dots=True,period=\"M\",title=title)\n",
    "def save_dict_to_json(dictionary, file_path):\n",
    "    try:\n",
    "        with open(file_path, 'a+') as file:\n",
    "            file.seek(0)\n",
    "            data = file.read()\n",
    "            if data:\n",
    "                file.seek(0, 2)  # Move the cursor to the end of the file\n",
    "                file.write(',')\n",
    "            json.dump(dictionary, file)\n",
    "            file.write('\\n')\n",
    "    except IOError as e:\n",
    "        print(f\"Error writing to JSON file: {str(e)}\")\n",
    "def saveDF(df, exp_desc, filename=\"UNDEFINED\"):\n",
    "    dict_exp_desc = dict()\n",
    "    current_timestamp = int(time.time())\n",
    "    exp_path = Path('../datasets/experiments_results/'+str(current_timestamp)+'_'+filename+'.csv')\n",
    "    df.to_csv(exp_path)\n",
    "    dict_exp_desc[str(current_timestamp)+'_'+filename+'.csv'] = exp_desc\n",
    "    file_path = \"../datasets/experiments_results/exp_description.json\"\n",
    "    save_dict_to_json(dict_exp_desc,file_path)\n",
    "    #with open(file_path, \"a\") as json_file:\n",
    "    #    json.dump(dict_exp_desc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Out of domain generalization (OODG): Station-level\n",
    "- to include/exclude lagPm10 values uncomment use_pm10_lags = True else False in Cell \"Select Features according to EDA\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DICT_METRICS = {}\n",
    "\n",
    "# all station test on Zagreb\n",
    "li_combinations = [[\"n\",\"z\"],[\"e\",\"z\"],[\"s\",\"z\"],[\"w\",\"z\"],[\"d\",\"z\"]]\n",
    "\n",
    "dict_predictions = {}\n",
    "\n",
    "df_results = pd.DataFrame()\n",
    "\n",
    "for combination in li_combinations:\n",
    "   \n",
    "    source = combination[0]\n",
    "    target = combination[1]\n",
    "        \n",
    "    print(fun.get_station_name_by_indice(source)+\" -> \"+fun.get_station_name_by_indice(target))\n",
    "\n",
    "    df_s_train_X, df_s_train_y, _, _, dict_s_info = fun.split_train_test_by_year(df=DICT_DF_STATIONS[source],pollutant_to_predict=\"pm10\",split_ratio=1.0,output=False)\n",
    "    _, _, df_t_test_X, df_t_test_y, dict_t_info = fun.split_train_test_by_year(df=DICT_DF_STATIONS[target],pollutant_to_predict=\"pm10\",split_ratio=1.0,output=False)\n",
    "    \n",
    "    # only consider equal features\n",
    "    li_columns = get_colname_intersection(dict_stations=DICT_DF_STATIONS,li_station_names=[source,target])\n",
    "    \n",
    "    li_columns = [x for x in li_columns if x not in ALL_POLLUTANTS]\n",
    "    df_s_train_X = df_s_train_X[li_columns]\n",
    "    df_t_test_X =  df_t_test_X[li_columns]\n",
    "   \n",
    "    \n",
    "    # Random Forest \n",
    "    regr = RandomForestRegressor(random_state=42,n_estimators=180,min_samples_split=5, min_samples_leaf=2, max_features=\"sqrt\", max_depth=60, bootstrap=True)    \n",
    "    regr.fit(X=df_s_train_X, y=df_s_train_y.values.ravel())\n",
    "    predictions_RF = regr.predict(df_t_test_X)\n",
    "    \n",
    "    calc_kpi(pred_result=predictions_RF,test_X=df_t_test_X,test_y=df_t_test_y,title=\"RF_pred\",output=False)\n",
    "        \n",
    "    dict_predictions[\"(\"+source+\"->\"+target+\")\"] = predictions_RF\n",
    "\n",
    "    df_results[\"(\"+source+\"->\"+target+\")\"] = pd.DataFrame(DICT_METRICS, index=['rmse', 'nrmse', 'mape'])\n",
    "    display(df_results)\n",
    "\n",
    "\n",
    "show_prediction_plot(df_y=df_t_test_y,dict_predictions=dict_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Out of domain generalization (OODG): City-level\n",
    "- Results displayed in Table 2 (in the paper: ref to paper is published)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers.legacy import Adam\n",
    "\n",
    "DICT_METRICS = {}\n",
    "\n",
    "li_all_station_graz = [['z','n', 'e', 's', 'w', 'd'],['z','e','s', 'w', 'd'],['z','n','s','w']]\n",
    "#li_all_station_graz = ['z','n','d']\n",
    "df_results = pd.DataFrame()\n",
    "\n",
    "dict_predictions = {}\n",
    "\n",
    "\n",
    "for li_combinations in li_all_station_graz:\n",
    "    \n",
    "\n",
    "    li_source = [x for x in li_combinations if x!=\"z\"]\n",
    "    target =  \"z\"\n",
    "\n",
    "    print(str([fun.get_station_name_by_indice(station) for station in li_source]) +\" -> \"+fun.get_station_name_by_indice(target))\n",
    "\n",
    "\n",
    "    df_global = model_fun.create_global_model(dict_stations=DICT_DF_STATIONS,li_station_combination=li_combinations)\n",
    "\n",
    "    df_s_train_X, df_s_train_y, df_t_test_X, df_t_test_y, _, _, dict_info = model_fun.split_train_test_by_station(df=df_global,\n",
    "                                                                                                                            li_stations=li_combinations,\n",
    "                                                                                                                            pollutant_to_predict=\"pm10\",\n",
    "                                                                                                                            station_test=target,\n",
    "                                                                                                                            station_validation=None,\n",
    "                                                                                                                            use_validation = False,\n",
    "                                                                                                                            output=True)\n",
    "    \n",
    "\n",
    "\n",
    "    # Random Forest\n",
    "    regr = RandomForestRegressor(random_state=42,n_estimators=180,min_samples_split=5, min_samples_leaf=2, max_features=\"sqrt\", max_depth=60, bootstrap=True)\n",
    "    regr.fit(X=df_s_train_X, y=df_s_train_y.values.ravel())\n",
    "    predictions_RF = regr.predict(df_t_test_X)\n",
    "\n",
    "\n",
    "    calc_kpi(pred_result=predictions_RF,test_X=df_t_test_X,test_y=df_t_test_y,title=\"RF_pred\",output=False)\n",
    "\n",
    "    strUniqueKey = str(li_source) +\" -> \"+target\n",
    "    dict_predictions[strUniqueKey] = predictions_RF\n",
    "\n",
    "    df_results[strUniqueKey] = pd.DataFrame(DICT_METRICS, index=['rmse', 'nrmse', 'mape'])\n",
    "    display(df_results)\n",
    "\n",
    "    \n",
    "\n",
    "show_prediction_plot(df_y=df_t_test_y,dict_predictions=dict_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison plot: city-level with and without lag values (only visible correctly if \"use_pm10_lag = true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dict_stations_nolag = DICT_DF_STATIONS.copy()\n",
    "dict_predictions_nolag = {}\n",
    "for station in dict_stations_nolag:\n",
    "        if \"pm10Lag\" in dict_stations_nolag[station].columns:\n",
    "            dict_stations_nolag[station].drop('pm10Lag',axis=1,inplace=True)\n",
    "            print(\"DELETE done for station '\"+fun.get_station_name_by_indice(station)+\"'\")\n",
    "\n",
    "\n",
    "df_global = model_fun.create_global_model(dict_stations=dict_stations_nolag,li_station_combination=li_combinations)\n",
    "\n",
    "\n",
    "df_s_train_X, df_s_train_y, df_t_test_X, df_t_test_y, _, _, dict_info = model_fun.split_train_test_by_station(df=df_global,\n",
    "                                                                                                                        li_stations=['z','n', 'e', 's', 'w', 'd'],\n",
    "                                                                                                                        pollutant_to_predict=\"pm10\",\n",
    "                                                                                                                        station_test=target,\n",
    "                                                                                                                        station_validation=None,\n",
    "                                                                                                                        use_validation = False,\n",
    "                                                                                                                        output=True)\n",
    "\n",
    "# Random Forest\n",
    "regr = RandomForestRegressor(random_state=42,n_estimators=180,min_samples_split=5, min_samples_leaf=2, max_features=\"sqrt\", max_depth=60, bootstrap=True)\n",
    "regr.fit(X=df_s_train_X, y=df_s_train_y.values.ravel())\n",
    "predictions_RF = regr.predict(df_t_test_X)\n",
    "\n",
    "\n",
    "calc_kpi(pred_result=predictions_RF,test_X=df_t_test_X,test_y=df_t_test_y,title=\"RF_pred\",output=False)\n",
    "\n",
    "strUniqueKey = str(li_source) +\" -> \"+target\n",
    "dict_predictions_nolag[strUniqueKey] = predictions_RF\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Station-level Transfer Learning\n",
    "- TrAdaBoostR2\n",
    "- Coral\n",
    "- KLIEP\n",
    "- Nearest Neighbour Weighting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DICT_METRICS = {}\n",
    "\n",
    "li_combinations = [[\"n\",\"z\"],[\"e\",\"z\"],[\"s\",\"z\"],[\"w\",\"z\"],[\"d\",\"z\"]]\n",
    "\n",
    "#li_combinations = [[\"n\",\"z\"]]\n",
    "\n",
    "for combination in li_combinations:\n",
    "    \n",
    "    \n",
    "    dict_predictions = {}\n",
    "    \n",
    "    source = combination[0]\n",
    "    target = combination[1]\n",
    "        \n",
    "    print(fun.get_station_name_by_indice(source)+\" -> \"+fun.get_station_name_by_indice(target))\n",
    "\n",
    "    df_s_train_X, df_s_train_y, _, _, dict_s_info = fun.split_train_test_by_year(df=DICT_DF_STATIONS[source],pollutant_to_predict=\"pm10\",split_ratio=1.0,output=False)\n",
    "    _, _, df_t_test_X, df_t_test_y, dict_t_info = fun.split_train_test_by_year(df=DICT_DF_STATIONS[target],pollutant_to_predict=\"pm10\",split_ratio=1.0,output=False)\n",
    "    \n",
    "    # only consider equal features\n",
    "    li_columns = get_colname_intersection(dict_stations=DICT_DF_STATIONS,li_station_names=[source,target])\n",
    "    \n",
    "    li_columns = [x for x in li_columns if x not in ALL_POLLUTANTS]\n",
    "    df_s_train_X = df_s_train_X[li_columns]\n",
    "    df_t_test_X =  df_t_test_X[li_columns]\n",
    "   \n",
    "    # years for injection\n",
    "    #years = [2019, 2017, 2016]\n",
    "    #years = [2019, 2014]\n",
    "    years = [2014,2015,2016,2017,2018,2019,2020]\n",
    "    months = [1,7]\n",
    "    \n",
    "    yt_samples_for_TL = select_by(df=df_t_test_y,y=years,m=months)\n",
    "    Xt_samples_for_TL = select_by(df=df_t_test_X,y=years,m=months)\n",
    "    \n",
    "    print(\"No. of injections: \", len(Xt_samples_for_TL))\n",
    "    \n",
    "    # Random Forest \n",
    "    regr = RandomForestRegressor(random_state=42,n_estimators=180,min_samples_split=5, min_samples_leaf=2, max_features=\"sqrt\", max_depth=60, bootstrap=True)    \n",
    "    regr.fit(X=df_s_train_X, y=df_s_train_y.values.ravel())\n",
    "    predictions_RF = regr.predict(df_t_test_X)\n",
    "    \n",
    "    # Random Forest with TrAdaBoostR2\n",
    "    model_ada_RF = TrAdaBoostR2(regr, Xt=Xt_samples_for_TL, yt=yt_samples_for_TL,n_estimators=20, random_state=0,verbose=0,lr=1)\n",
    "    model_ada_RF.fit(df_s_train_X, df_s_train_y)\n",
    "    predictions_RF_TL = model_ada_RF.predict(df_t_test_X)\n",
    "    \n",
    "    # Random Forest with Coral\n",
    "    model_coral_RF = CORAL(regr, lambda_=1e-3, random_state=0,verbose=0)\n",
    "    model_coral_RF.fit(df_s_train_X, df_s_train_y, df_t_test_X)\n",
    "    predictions_coral_RF = model_coral_RF.predict(df_t_test_X)\n",
    "    \n",
    "    # # Random Forest with KLIEP\n",
    "    model_kliep = KLIEP(regr, Xt=df_t_test_X, kernel=\"polynomial\", gamma=[10**(i-4) for i in range(20)], random_state=0,verbose=0)\n",
    "    model_kliep.fit(df_s_train_X,df_s_train_y)\n",
    "    predictions_kliep_RF = model_kliep.predict(df_t_test_X)\n",
    "    \n",
    "    # # Random Forest with Nearest Neighbour Weighting\n",
    "    model_NN = NearestNeighborsWeighting(regr, n_neighbors=100, Xt=df_t_test_X, random_state=0)\n",
    "    model_NN.fit(X=df_s_train_X,y=df_s_train_y,Xt=Xt_samples_for_TL,yt=yt_samples_for_TL)\n",
    "    predictions_NN_RF = model_NN.predict(df_t_test_X)\n",
    "    \n",
    "    # uncomment to test a scenario\n",
    "    \n",
    "    \n",
    "    calc_kpi(pred_result=predictions_RF,test_X=df_t_test_X,test_y=df_t_test_y,title=\"RF_pred\",output=False)\n",
    "    calc_kpi(pred_result=predictions_NN_RF,test_X=df_t_test_X,test_y=df_t_test_y,title=\"RF_NNW_pred\",output=False)\n",
    "    calc_kpi(pred_result=predictions_kliep_RF,test_X=df_t_test_X,test_y=df_t_test_y,title=\"RF_kliep_pred\",output=False)\n",
    "    calc_kpi(pred_result=predictions_coral_RF,test_X=df_t_test_X,test_y=df_t_test_y,title=\"RF_coral_pred\",output=False)\n",
    "    calc_kpi(pred_result=predictions_RF_TL,test_X=df_t_test_X,test_y=df_t_test_y,title=\"RF_ada_pred\",output=False)\n",
    "\n",
    "   \n",
    "    dict_predictions[\"(\"+source+\"->\"+target+\")\"+\"OoDG_pred\"] = predictions_RF\n",
    "    dict_predictions[\"(\"+source+\"->\"+target+\")\"+\"TrAdaBR2:pred\"] = predictions_RF_TL\n",
    "    #dict_predictions[\"(\"+source+\"->\"+target+\")\"+\"mlp_pred\"] = prediction_mlp_noTL\n",
    "    dict_predictions[\"(\"+source+\"->\"+target+\")\"+\"CORAL\"] = predictions_coral_RF\n",
    "    \n",
    "    \n",
    "    dict_predictions[\"OoDG_pred\"] = predictions_RF\n",
    "    dict_predictions[\"TrAdaBR2_pred\"] = predictions_RF_TL\n",
    "    dict_predictions[\"CORAL_pred\"] = predictions_coral_RF\n",
    "    dict_predictions[\"NNW\"] =        predictions_NN_RF\n",
    "    dict_predictions[\"KLIEP_pred\"] = predictions_kliep_RF\n",
    "    \n",
    "    #display(fun.calc_exceeding_days(df_y=df_t_test_y,predictions=predictions_RF))\n",
    "\n",
    "    \n",
    "    \n",
    "    df_results = pd.DataFrame(DICT_METRICS, index=['rmse', 'nrmse', 'mape'])\n",
    "    display(df_results)\n",
    "\n",
    "#dict_predictions\n",
    "\n",
    "show_prediction_plot(df_y=df_t_test_y,dict_predictions=dict_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### City-level transfer learning\n",
    "- TrAdaBoostR2\n",
    "- Coral\n",
    "- KLIEP\n",
    "- Nearest Neighbour Weighting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers.legacy import Adam\n",
    "\n",
    "DICT_METRICS = {}\n",
    "\n",
    "\n",
    "li_all_station_graz = ['z','w', 'n', 's' ,'e', 'd']\n",
    "\n",
    "    \n",
    "dict_predictions = {}\n",
    "\n",
    "\n",
    "li_source = [x for x in li_all_station_graz if x!=\"z\"]\n",
    "target =  \"z\"\n",
    "\n",
    "print(str([fun.get_station_name_by_indice(station) for station in li_source]) +\" -> \"+fun.get_station_name_by_indice(target))\n",
    "\n",
    "\n",
    "df_global = model_fun.create_global_model(dict_stations=DICT_DF_STATIONS,li_station_combination=li_all_station_graz)\n",
    "df_s_train_X, df_s_train_y, df_t_test_X, df_t_test_y, _, _, dict_info = model_fun.split_train_test_by_station(df=df_global,\n",
    "                                                                                                                        li_stations=li_all_station_graz,\n",
    "                                                                                                                        pollutant_to_predict=\"pm10\",\n",
    "                                                                                                                        station_test=target,\n",
    "                                                                                                                        station_validation=None,\n",
    "                                                                                                                        use_validation = False,\n",
    "                                                                                                                        output=True)\n",
    "\n",
    "\n",
    "# years for injection\n",
    "#li_years = [[2019, 2017, 2016],[2017],[2019],[2014, 2019],[2014, 2017, 2019]]\n",
    "   \n",
    "#li_years = [[2014],[2015],[2016],[2017],[2018],[2019],[2020]]\n",
    "#years = [2014,2016,2018,2020]\n",
    "#years = [2015,2017,2019]\n",
    "\n",
    "years = [2014,2015,2016,2017,2018,2019,2020]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#years = [2014,2017,2019]\n",
    "#years = [2019, 2017, 2016]\n",
    "months = [1,7]\n",
    "\n",
    "yt_samples_for_TL = select_by(df=df_t_test_y,y=years,m=months)\n",
    "Xt_samples_for_TL = select_by(df=df_t_test_X,y=years,m=months)\n",
    "\n",
    "print(\"Years\",str(years))\n",
    "print(\"No. of injections: \", len(Xt_samples_for_TL))\n",
    "no_injections = len(Xt_samples_for_TL)\n",
    "\n",
    "# Random Forest\n",
    "regr = RandomForestRegressor(random_state=42,n_estimators=180,min_samples_split=5, min_samples_leaf=2, max_features=\"sqrt\", max_depth=60, bootstrap=True)\n",
    "regr.fit(X=df_s_train_X, y=df_s_train_y.values.ravel())\n",
    "predictions_RF = regr.predict(df_t_test_X)\n",
    "\n",
    "\n",
    "# Random Forest with TrAdaBoostR2\n",
    "model = TrAdaBoostR2(regr, Xt=Xt_samples_for_TL, yt=yt_samples_for_TL,n_estimators=20, random_state=0,verbose=0,lr=2)\n",
    "model.fit(df_s_train_X, df_s_train_y)\n",
    "predictions_RF_TL = model.predict(df_t_test_X)\n",
    "\n",
    "\n",
    "# Random Forest with Coral\n",
    "model_coral_RF = CORAL(regr, lambda_=1e-3, random_state=0,verbose=0)\n",
    "model_coral_RF.fit(df_s_train_X, df_s_train_y, df_t_test_X)\n",
    "predictions_coral_RF = model_coral_RF.predict(df_t_test_X)\n",
    "\n",
    "# # Random Forest with KLIEP\n",
    "model_kliep = KLIEP(regr, Xt=df_t_test_X, kernel=\"polynomial\", gamma=[10**(i-4) for i in range(20)], random_state=0,verbose=0)\n",
    "model_kliep.fit(df_s_train_X,df_s_train_y)\n",
    "predictions_kliep_RF = model_kliep.predict(df_t_test_X)\n",
    "\n",
    "# # Random Forest with Nearest Neighbour Weighting\n",
    "model_NN = NearestNeighborsWeighting(regr, n_neighbors=100, Xt=df_t_test_X, random_state=0)\n",
    "model_NN.fit(X=df_s_train_X,y=df_s_train_y,Xt=Xt_samples_for_TL,yt=yt_samples_for_TL)\n",
    "predictions_NN_RF = model_NN.predict(df_t_test_X)\n",
    "\n",
    "\n",
    "calc_kpi(pred_result=predictions_RF,test_X=df_t_test_X,test_y=df_t_test_y,title=\"RF_pred\",output=False)\n",
    "calc_kpi(pred_result=predictions_NN_RF,test_X=df_t_test_X,test_y=df_t_test_y,title=\"RF_NNW_pred\",output=False)\n",
    "calc_kpi(pred_result=predictions_kliep_RF,test_X=df_t_test_X,test_y=df_t_test_y,title=\"RF_kliep_pred\",output=False) \n",
    "calc_kpi(pred_result=predictions_coral_RF,test_X=df_t_test_X,test_y=df_t_test_y,title=\"RF_coral_pred\",output=False)\n",
    "calc_kpi(pred_result=predictions_RF_TL,test_X=df_t_test_X,test_y=df_t_test_y,title=\"RF_ada_pred\",output=False)\n",
    "\n",
    "\n",
    "dict_predictions[\"Oo_DG\"] = predictions_RF\n",
    "dict_predictions[\"TrABR2\"] = predictions_RF_TL\n",
    "dict_predictions[\"CORAL_pred\"] = predictions_coral_RF\n",
    "dict_predictions[\"NNW_pred\"] = predictions_NN_RF\n",
    "dict_predictions[\"KLIEP_pred\"] = predictions_kliep_RF\n",
    "\n",
    "\n",
    "\n",
    "df_results = pd.DataFrame(DICT_METRICS, index=['rmse', 'nrmse', 'mape'])\n",
    "display(df_results)\n",
    "\n",
    "\n",
    "\n",
    "#show_prediction_plot(df_y=df_t_test_y,dict_predictions=dict_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Station-level transfer estimate number of injections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DICT_METRICS = {}\n",
    "\n",
    "li_combinations = [[\"n\",\"z\"],[\"e\",\"z\"],[\"s\",\"z\"],[\"w\",\"z\"],[\"d\",\"z\"]]\n",
    "li_combinations = [[\"d\",\"z\"]]\n",
    "\n",
    "#  true -> injection data from target domain, false -> injection data from source domain,\n",
    "unsupervised = False\n",
    "\n",
    "for combination in li_combinations:\n",
    "    \n",
    "    dict_predictions = {}\n",
    "    \n",
    "    source = combination[0]\n",
    "    target = combination[1]\n",
    "        \n",
    "    print(fun.get_station_name_by_indice(source)+\" -> \"+fun.get_station_name_by_indice(target))\n",
    "\n",
    "    df_s_train_X, df_s_train_y, _, _, dict_s_info = fun.split_train_test_by_year(df=DICT_DF_STATIONS[source],pollutant_to_predict=\"pm10\",split_ratio=1.0,output=False)\n",
    "    _, _, df_t_test_X, df_t_test_y, dict_t_info = fun.split_train_test_by_year(df=DICT_DF_STATIONS[target],pollutant_to_predict=\"pm10\",split_ratio=1.0,output=False)\n",
    "    \n",
    "    # only consider equal features\n",
    "    li_columns = get_colname_intersection(dict_stations=DICT_DF_STATIONS,li_station_names=[source,target])\n",
    "    # get rid off pollutants\n",
    "    li_columns = [x for x in li_columns if x not in ALL_POLLUTANTS]\n",
    "  \n",
    "    df_s_train_X = df_s_train_X[li_columns]\n",
    "    df_t_test_X =  df_t_test_X[li_columns]\n",
    "   \n",
    "    # years for injection\n",
    "   \n",
    "    year_month_combi = [([],[]), ([2014,2015,2016,2017,2018,2019,2020],[1,2,6,7]),\n",
    "                                 ([2014,2015,2016,2017,2018,2019,2020],[1,7]),\n",
    "                                 ([2016,2017,2019],[1,7]),\n",
    "                                 ([2014,2017,2019],[1,7]),\n",
    "                                 ([2024,2019],[1,7]),\n",
    "                                 ([2017],[1,7]),\n",
    "                                 ([2019],[1,7])]\n",
    "    \n",
    "    #years_combi = [[],[2014,2018],[2019,2017,2016],[2014,2017,2020]]\n",
    "    #years_combi = [[],[2014,2018],[2019,2017,2016],[2014,2017,2020]]\n",
    "    \n",
    "    experiment_number = 1\n",
    "    for year_month in year_month_combi:\n",
    "        years = year_month[0]\n",
    "        months = year_month[1]\n",
    "        title = \"\"\n",
    "        if unsupervised:\n",
    "              # use stations Graz\n",
    "            df_inj = DICT_DF_STATIONS[\"s\"]\n",
    "            #yt_samples_for_TL = select_by(df=df_s_train_y,y=years,m=months)\n",
    "            #Xt_samples_for_TL = select_by(df=df_s_train_X,y=years,m=months)\n",
    "            print(\"here\")\n",
    "            yt_samples_for_TL = select_by(df=df_inj,y=years,m=months)\n",
    "            yt_samples_for_TL = yt_samples_for_TL[[\"pm10\"]]\n",
    "            Xt_samples_for_TL = select_by(df=df_inj,y=years,m=months)\n",
    "            Xt_samples_for_TL = Xt_samples_for_TL[li_columns]\n",
    "        else:\n",
    "             # use station Zagreb\n",
    "            yt_samples_for_TL = select_by(df=df_t_test_y,y=years,m=months)\n",
    "            Xt_samples_for_TL = select_by(df=df_t_test_X,y=years,m=months)\n",
    "\n",
    "        # Random Forest with TrAdaBoostR2\n",
    "        regr = RandomForestRegressor(random_state=42,n_estimators=180,min_samples_split=5, min_samples_leaf=2, max_features=\"sqrt\", max_depth=60, bootstrap=True)\n",
    "        \n",
    "        # no transfer\n",
    "        if years == []:\n",
    "            print(\"no transfer\")\n",
    "            regr.fit(df_s_train_X, df_s_train_y.values.ravel())\n",
    "            predictions_RF_TL = regr.predict(df_t_test_X)\n",
    "            title = \"RF_pred_\"\n",
    "        else:\n",
    "            print(\"transfer\")\n",
    "            title = \"TL_RF_pred_\"\n",
    "            model = TrAdaBoostR2(regr, Xt=Xt_samples_for_TL, yt=yt_samples_for_TL,n_estimators=20, random_state=0,verbose=0,lr=1)\n",
    "            model.fit(df_s_train_X, df_s_train_y)\n",
    "            predictions_RF_TL = model.predict(df_t_test_X)\n",
    "       \n",
    "        add_info_station_level = (len(Xt_samples_for_TL),years, months)\n",
    "        calc_kpi(pred_result=predictions_RF_TL,test_X=df_t_test_X,test_y=df_t_test_y,title=title+str(experiment_number),output=True, additional_info=add_info_station_level)\n",
    "        \n",
    "        \n",
    "        experiment_number+=1\n",
    "\n",
    "    break\n",
    "        \n",
    "df_results = pd.DataFrame(DICT_METRICS, index=['rmse', 'nrmse', 'mape', 'inj_samples', 'years', 'months'])\n",
    "display(df_results)\n",
    "\n",
    "desc = \"RF\" +\";\"+ \"TrAdaBoostR2\" +\";\"+ source + \"->\"+ target +\";\"+ str(months) +\";\"+ (\"unsupervised\" if unsupervised else \"supervised\")\n",
    "saveDF(df=df_results,exp_desc=desc,filename=\"SL_month_inj\")\n",
    "\n",
    "# select rows and create dataframe with inj_samples as index and nrmse as values\n",
    "row_index_1 = 0\n",
    "row_index_2 = 1  \n",
    "row_index_3 = 3  \n",
    "\n",
    "transposed_df = df_results.iloc[[row_index_1, row_index_2, row_index_3]].transpose()\n",
    "transposed_df.set_index(\"inj_samples\", inplace=True)\n",
    "display(transposed_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model purely on Zagreb\n",
    "- First 100% data availability\n",
    "- Second same data availability as for TL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DICT_METRICS = {}\n",
    "\n",
    "dataZagrebSelected = select_by(DICT_DF_STATIONS[\"z\"],[2014,2015,2016,2017,2018,2019,2020],[1,2,3,5,7,6])\n",
    "dataZagrebSelected = select_by(DICT_DF_STATIONS[\"z\"],[2014,2015,2016,2017,2018,2019,2020],[1,7])\n",
    "\n",
    "# all data:\n",
    "#dataZagrebSelected = DICT_DF_STATIONS[\"z\"]\n",
    "\n",
    "\n",
    "df_train_X, df_train_y, df_test_X, df_test_y, dict_s_info = fun.split_train_test_by_year(\n",
    "                                                                    df=dataZagrebSelected,\n",
    "                                                                    pollutant_to_predict=\"pm10\",\n",
    "                                                                    split_ratio=0.8,output=True)\n",
    "\n",
    "\n",
    "\n",
    "# # Random Forest \n",
    "regr = RandomForestRegressor(random_state=42,n_estimators=180,min_samples_split=5, min_samples_leaf=2, max_features=\"sqrt\", max_depth=60, bootstrap=True)    \n",
    "regr.fit(X=df_train_X, y=df_train_y.values.ravel())\n",
    "predictions_RF = regr.predict(df_test_X)\n",
    "\n",
    "calc_kpi(pred_result=predictions_RF,test_X=df_test_X,test_y=df_test_y,title=\"RF_pred\",output=True)\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PMForecastingVenV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
